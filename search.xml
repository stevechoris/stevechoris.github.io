<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[机器学习和数据挖掘常用算法整理]]></title>
      <url>%2F2017%2F04%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95%E6%95%B4%E7%90%86%2F</url>
      <content type="text"><![CDATA[&emsp; 机器学习和数据挖掘常用算法整理 本文目录主要参考《机器学习实战》(Peter Harrington著) 一书。 [ ] 计算广告学 [ ] SVD、PCA、LDA 一 分类Naive Bayes$$P(A∩B)=P(A)P(B|A)=P(B)P(A|B) =&gt; $$$$P(A|B)=P(B|A)P(A)/P(B)$$对于给出的待分类项，求解在此项出现的条件下各个目标类别出现的条件概率，哪个最大，就认为此待分类项属于哪个类别。假设样本$x=(a_1,a_2,a_3,…a_n)$(特征独立)，分类目标$Y={y_1,y_2,y_3,y_4..y_n}$，通过$max(P(y_1|x),P(y_2|x),P(y_3|x)..P(y_n|x))$分类。而根据贝叶斯公式：$$P(y_i|x)=p(x|y_i)P(y_i)/P(x) =&gt; max(P(x|y_i)p(y_i))$$$$P(x|y_i)p(y_i)=p(y_i)*\prod{P(ai|y_i)}$$ 属性特征特征为离散值时直接统计即可（表示统计概率）特征为连续值的时候假定特征符合高斯分布:g(x,n,u) p(ak|y_i)=g(xk,ni,ui) Laplace校准(拉普拉斯校验)当某个类别下某个特征划分没有出现时，会有P(a|y)=0，就是导致分类器质量降低，所以此时引入Laplace校验，就是对每类别下所有划分的计数加1。 优缺点优点：模型所需估计的参数很少，具有稳定的计算效率，对小规模的数据表现很好。对缺失数据不敏感，适合多分类任务，适合增量式训练。缺点：NBC前提是假设属性之间相互独立，但在实际应用中往往不满足。算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification) 遇到特征之间不独立问题 [ ] 参考改进的贝叶斯网络，使用DAG来进行概率图的描述 Logistic回归 美团 - 逻辑回归简介 k-近邻算法对预测数据，找到最近k个实例，根据k个实例的距离和属性获取预测值。 1 三要素：距离的度量（常见的距离度量有欧式距离，马氏距离等）k值的选择分类决策规则 （多数表决规则） 2 k值的选择k值越小,模型越复杂，容易过拟合；k值越大，模型越简单，如k=N 一般k会取一个较小的值，然后用过交叉验证来确定：将样本划分一部分出来为预测样本，比如95%训练，5%预测，然后k分别取1，2，3，4，5之类的，进行预测，计算最后的分类误差，选择误差最小的k 3 优缺点优点: 思想简单，理论成熟，既可以用来做分类也可以用来做回归；可用于非线性分类；训练时间复杂度为O(n)；准确度高，对数据没有假设，对outlier不敏感； 缺点: 计算量大；样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）;需要大量的内存； 4 KD树 k-d tree算法 ==== KD树进行KNN查找通过KD树的搜索找到与搜索目标最近的点，这样KNN的搜索就可以被限制在空间的局部区域上了，可以大大增加效率。 KD树搜索的复杂度当实例随机分布的时候，搜索的复杂度为log(N)，N为实例的个数KD树更加适用于实例数量远大于空间维度的KNN搜索，如果实例的个数与空间维度差不多时，它的效率基于等于线性扫描。 DT决策树 机器学习算法-决策树理论 ID3 熵和信息增益的概念： 算法步骤： 针对当前的集合，计算每个特征的信息增益 然后选择信息增益最大的特征作为当前节点的决策特征 根据决策特征，把集合样本划分到不同的子节点 然后继续对每个子节点进行递归，直到每个集合只有一个类别 信息熵：整个属性的熵:为各个分支的比例与各自熵的加权求和信息增益:表示分类目标的熵减去当前属性的熵，增益越大，分类能力越强 (这里前者叫做经验熵，表示数据集分类C的不确定性，后者就是经验条件熵，表示在给定A的条件下对数据集分类C的不确定性，两者相减叫做互信息，决策树的增益等价于互信息) 损失函数:设树的叶子节点个数为T，t为其中一个叶子节点，该叶子节点有$Nt$个样本，其中k类的样本有$N{t_k}$个，H(t)为叶子节点上的经验熵，则损失函数定义为$$Ct(T)=\sum(Nt*H(t))+ \lambda|T|$$其中$H(t)=\sum(\frac{N{t_k}}{Nt}*log(\frac{N{t_k}}{N_t}))$代入可以得到$C_t(T)=\sum(Nt*\sum(N{tk}*log(\frac{N{t_k}}{N_t}))+\lambda|T|$最终有$C_t(T)=C(T)+ \lambda|T|$$\lambda|T|$为正则化项，$\lambda$是用于调节比率决策树的生成只考虑了信息增益 C4.5 信息增益律的概念 优缺点：它是在ID3基础上的一个改进，准确率高，但是子构造树的过程中需要进行多次的扫描和排序，所以它的运算效率较低 基尼指数Gini index基尼指数主要在CART算法中用到，随机森林中用到的属性划分标准也是它。Gini index划分是二元的，它度量的是数据分区或训练元组集D的不纯度，表示的是一个随机选中的样本在子集中被分错的可能性。计算方式如下： $$ Gini(D)=1−\Sigma p^2_i，其中，pi是D中元组数以Ci类的概率，对m个类计算和。$$ Gini指数越大，不纯度越大，越不容易区分。假设A有v个不同的值出现在特征D中，它的二元划分有$2^v−2$种（除去自己和空集）。当考虑二元划分裂时，计算每个结果分区的不纯度加权和。比如A有两个值，则特征D被划分成D1和D2,这时Gini指数为： $$Gini_A(D) = \frac{D_1}{D} Gini(D_1) + \frac{D_2}{D} Gini(D_2)$$ 上面的式子表示的是不确定性的大小。对于每个属性，考虑每种可能的二元划分，对于离散值属性，选择该属性产生最小Gini指数的自己作为它的分裂信息。 AdaBoost元算法提高分类性能 boosting是一种跟bagging很类似的技术。不论是在boosting还是bagging当中，所使用的多个分类器类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。由于boosting分类的结果是基于所有分类器的加权求和结果的，因此boosting与bagging不太一样。bagging中的分类器权重是相等的，而boosting中的分类器权重并不相等，每个权重代表的是对应分类器在上一轮迭代中的成功度。 集成算法（Ensemble algorithms）集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。 该算法主要的问题是要找出哪些较弱的模型可以结合起来，以及结合的方法。这是一个非常强大的技术集，因此广受欢迎。 例子 Bagging：Bootstrapped Aggregation） 随机森林（Random Forest） Boosting： AdaBoost 梯度提升回归树（Gradient Boosted Regression Trees，GBRT） 优缺点优点：当先最先进的预测几乎都使用了算法集成。它比使用单个模型预测出来的结果要精确的多** 缺点：需要大量的维护工作 Bagging 从N样本中有放回的采样N个样本 对这N个样本在全属性上建立分类器(CART,SVM) 重复上面(1,2)的步骤，建立m个分类器 预测的时候使用投票的方法得到结果 Boostingboosting在训练的时候会给样本加一个权重，然后使loss function尽量去考虑那些分错类的样本（比如给分错类的样本的权重值加大），根据当前分类器在数据集上面的预测准确率，给当前分类器分配一个权重$\alpha$。 AdaBoost 参考机器学习实战第七章 能否使用弱分类器和多个实例来构建一个强分类器？这是一个非常有趣的理论问题。这里的“弱”意味着分类器的性能比随机猜测略好，但是也不会好太多。这就是说，在二分类情况下弱分类器的错误率会高于50%，而“强”分类器的错误率将会低很多。AdaBoost算法即脱胎于上述理论问题。 AdaBoost是adaptive boosting (自适应boosting）的缩写，其运行过程如下：训练数据中的每一个样本，并赋予其一个权重，这些权重构成了向量D。一开始，这些权重都初始化成相等值。首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分队的样本的权重将会降低，而第一次分错的样本的权重将会提高。为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。其中，错误率$\varepsilon $的定义为： $$\varepsilon=\frac{未正确分类的样本数目}{所有样本数目} $$而alpha的计算公式如下：$$\alpha=\frac{1}{2}\ln{\frac{1-\varepsilon}{\varepsilon}}$$ AdaBoost算法的流程如图所示： 计算出alpha值之后，可以对权重向量进行D进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。D的计算方法如下。如果某个样本被正确分类，那么该样本的权重更改为：$$D{i}^{t+1}=\frac{D{i}^{(t)}e^{-\alpha}}{Sum(D)}$$而如果一个样本被错分，那么该样本的权重更改为：$$D{i}^{t+1}=\frac{D{i}^{(t)}e^{\alpha}}{Sum(D)}$$在计算出D之后，AdaBoost又开始进入下一轮迭代。AdaBoost算法会不断地重复训练和调整权重的过程，直到训练错误率为0或者分类器的树木达到用户的指定值为止。下面学习一个基于单层决策树的AdaBoost算法代码，并绘制AdaBoost算法流程： 构建单层决策树伪代码： 构建单层决策树代码： 完整AdaBoost伪代码： GBDTGBDT（MART） 迭代决策树入门教程 | 简介 - w28971023的专栏 - 博客频道 - CSDN.NET 随机森林 随机森林原理介绍： 预测过程将预测样本输入到K颗树分别进行预测如果是分类问题，直接使用投票的方式选择分类频次最高的类别如果是回归问题，使用分类之后的均值作为结果 参数问题 一般取m=sqrt(M) 关于树的个数K，一般都需要成百上千，但是也有具体的样本有关（比如特征数量） 树的最大深度，（太深可能可能导致过拟合？？） 节点上的最小样本数、最小信息增益 泛化误差估计使用oob（out-of-bag）进行泛化误差的估计，将各个树的未采样样本作为预测样本（大约有36.8%），使用已经建立好的森林对各个预测样本进行预测，预测完之后最后统计误分得个数占总预测样本的比率作为RF的oob误分率。 SVM支持向量机 参考： 最优化理论与KTT条件2. 支持向量机：Duality 损失函数经验损失函数:$sigma(1-y_i(w*x_i+b))$ (注意，如果该值小于0时直接取0即可)合页损失函数：$\sigma(1-y_i(wi+b)) + lambda||w||^2$ 后面的是L2正则项 为什么要引入对偶算法对偶问题往往更加容易求解(结合拉格朗日和kkt条件)可以很自然的引用核函数（拉格朗日表达式里面有内积，而核函数也是通过内积进行映射的） 核函数将输入特征x（线性不可分）映射到高维特征R空间，可以在R空间上让SVM进行线性可以变，这就是核函数的作用多项式核函数:$K(x,z)=(x*z+1)^p$高斯核函数:$K(x,z)=exp(-(x-z)^2/a^2)$ a为均值字符串核函数：好像用于文本匹配、检索之类的，不懂 SVM优缺点 优点： 使用核函数可以向高维空间进行映射使用核函数可以解决非线性的分类分类思想很简单，就是将样本与决策面的间隔最大化分类效果较好 缺点： 对大规模数据训练比较困难，因为它是用二次规划来求解的无法直接支持多分类，但是可以使用间接的方法来做 SMOSMO是用于快速求解SVM的它选择凸二次规划的两个变量，其他的变量保持不变，然后根据这两个变量构建一个二次规划问题，这个二次规划关于这两个变量解会更加的接近原始二次规划的解，通过这样的子问题划分可以大大增加整个算法的计算速度，关于这两个变量：其中一个是严重违反KKT条件的一个变量另一个变量是根据自由约束确定，好像是求剩余变量的最大化来确定的。 SVM多分类问题 直接法: 直接在目标函数上进行修改，将多个分类面的参数求解合并到一个最优化问题中，通过求解该优化就可以实现多分类（计算复杂度很高，实现起来较为困难） 间接法: 一对多其中某个类为一类，其余n-1个类为另一个类,这方式共需要训练n个分类器，最后在测试的时候将测试样本经过这4个分类器f1(x),f2(x),f3(x)和 f4(x),取其最大值为分类器(这种方式由于是1对M分类，会存在偏置，很不实用) 一对一(libsvm实现的方式)任意两个类都训练一个分类器，那么n个类就需要n*(n-1)/2个svm分类器。在预测的将测试样 本通过这些分类器之后进行投票选择最终结果。（这种方法虽好，但是需要n*(n-1)/2个分类器代价太大，不过有好像使用循环图来进行改进） 二 回归：利用回归预测数值型数据预测数值型数据：回归LR逻辑回归和线性回归逻辑回归是一个线性的二分类模型，主要是计算在某个样本特征下事件发生的概率，比如根据用户的浏览购买情况作为特征来计算它是否会购买这个商品，抑或是它是否会点击这个商品。 由一个线性和函数与一个sigmod函数组成，训练LR就是训练线性和函数的各个权重值w。 一般使用最大似然法来估计，比如y_i=1的概率是pi,则y_i=0的概率是1-pi，那么观测概率为$p(y_i)=pi^y_i (1-pi)^(1-y_i)$最大似然函数为$（hw(x_i)^y_i(1-hw(x_i))^(1-y_i)）$连乘，对这个似然函数取对数之后就会得到 的表达式$L(w)=sigma(y_ilog(hw(x_i))-(1-y_i)log(1-hw(x_i)))=sigma(y_i(wx_i)- log(1+exp(wx_i)))$估计这个L(w)的极大值就可以得到w的估计值。最大似然函数的最优化问题，通常采用随机梯度下降法和拟牛顿迭代法来进行优化 hw为lr函数，y_i为二分类结果0或1，x_i为特征向量 梯度下降法如果$hw(x)=1/(1-e^(-wx))$，则$cost function=-1/m sigma(y_ilog(hw(x_i)+(1-y_i)log(1-hw(x_i)))=j(w)$这里就成了就$min(j(w))$所以更新w的过程为$w:=w-lameaj(w)’ (求导)$ =&gt; $w:=w-lamea 1/m*sigmam-y_i)*x_i)$直到j(w)不能再的时候停止 最大问题: 会陷入局部最优，并且每次在对当前样本计算cost的时候都需要去遍历全部样本才能得到cost值，这样计算速度就会慢很多（虽然在计算的时候可以转为矩阵乘法去更新整个w值）所以现在好多框架（mahout）中一般使用随机梯度下降法，它在计算cost的时候只计算当前的代价，最终cost是在全部样本迭代一遍之求和得出，还有他在更新当前的参数w的时候并不是依次遍历样本，而是从所有的样本中随机选择一条进行计算，它方法收敛速度快（一般是使用最大迭代次数），并且还可以避免局部最优，并且还很容易并行（使用参数服务器的方式进行并行）这里SGD可以改进的地方就是使用动态的梯度值alpha=0.04*(1.0+n+i)+Rate 其他优化方法拟牛顿法（记得是需要使用Hessian矩阵和cholesky分解）BFGSL-BFGS 优缺点：无需选择学习率α，更快，但是更复杂 过拟合问题 减少feature个数（人工定义留多少个feature、算法选取这些feature） 正则化（留下所有的feature，但对于部分feature定义其parameter非常小），在cost上加$lamea(sigma(w^2))$，同时w的更新变为$w:=w-rate 1/m\sigmam-y_i)x_i+ （lamea/m)w$。注意：这里的w0不受正则化影响 正则化算法（Regularization Algorithms）它是另一种方法（通常是回归方法）的拓展，这种方法会基于模型复杂性对其进行惩罚，它喜欢相对简单能够更好的泛化的模型。 例子：岭回归（Ridge Regression）最小绝对收缩与选择算子（LASSO）GLASSO弹性网络（Elastic Net）最小角回归（Least-Angle Regression） 优缺点其惩罚会减少过拟合总会有解决方法 惩罚会造成欠拟合很难校准 多分类softmaxsoftmax: 假设离散型随机变量Y的取值集合是{1,2,..,k},则多分类的LR为$P(Y=a|x)=exp(wax)/(1-1到k求和(wkx)) 1&lt;a&lt;k$这里会输出当前样本下属于哪一类的概率，并且满足全部概率加起来=1 关于softmax和k个LR的选择 类别之间是否互斥（比如音乐只能属于古典音乐、乡村音乐、摇滚月的一种），用softmax 类别之间有联系（比如一首歌曲可能有影视原声，也可能包含人声，或者是舞曲），用k个LR更为合适 优缺点优点：实现简单；分类时计算量非常小，速度很快，存储资源低；缺点：容易欠拟合，一般准确度不太高。只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；Logistic Regression–逻辑回归算法汇总Stanford机器学习—第三讲. 逻辑回归和过拟合问题的解决 logistic Regression &amp; RegularizationSoftmax回归 Cart分类回归树 线性回归包含了一些强大的方法，但这些方法创建的模型需要拟合所有的样本点（局部加权线性回归除外）。当数据拥有众多特征并且特征之间关系十分复杂时，构建全局模型的想法就显得太难了，也略显笨拙。一种可行的方法是将数据集切分成多份易建模的数据，然后利用线性回归技术建模。如果首次切分后仍然难以拟合线性模型就继续切分。在这种切分方式下，树结构和回归法就相当有用。 树回归优点：可以对复杂和非线性的数据建模缺点：结果不易理解适用数据类型：数值型和标称型数据 1 与ID3算法的比较 切分方式过于迅速：ID3的做法是每次选取当前最佳的特征来分割数据，并按照特征的所有可能取值来切分。也就是说，如果一个特征有4种取值，那么数据将被切成4份。一旦按某个特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另外一种方法是二元切分法，即每次把数据集切成两份。如果数据的某特征值等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。 处理连续值特征：，ID3算法还存在另一个问题，它不能直接处理连续型特征值。只有事先将连续型特征转换成离散型，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则易于对树构建过程进行调整以处理连续型特征。具体处理方法是：如果特征值大于给定值就走左子树，否则就走右子树。 节省时间：二元切分法节省了树的构建时间。但这点意义也不是特别大，因为这些树构建一般是离线完成，时间并非需要重点关注的因素。 2 构建CART回归树 回归树与分类树的思路类似，但也节点的数据类型不是离散型，而是连续型。 树节点存储结构： 待切分的特征 待切分的特征值 右子树。当不再需要切分的时候，也可以是单个值。 左子树。与右子树类似。 Pyton定义： 后面将介绍两种树的构建：第一种是回归树（regression tree），其每个叶节点包含单个值；第二种是模型树（model tree），其每个也节点包含一个线性方程。 先给出两种树构建算法中的一些共用代码。函数createTree()的伪代码大致如下： 找到最佳的待切分特征 如果该节点不能再分，将该节点存为叶节点 执行二元切分 在右子树调用createTree()方法 在左子树调用createTree()方法 binSplitDataSet切分函数和createTree创建树函数代码： 回归树中，假设叶节点是常数值，这种策略认为数据中的复杂关系可以用树结构来概括。那为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。这里涉及到如何像分类树一样计算连续型数值的混乱度呢？实际的做法就是计算总方差。总方差可以通过均方差乘以数据集中样本点的个数来得到。 chooseBestSplit()函数这个函数对于给定的某个误差计算方法，可以找到数据集上最佳的二元切分方式。另外，该函数还要确定什么时候停止切分，一旦停止切分会生成一个叶节点。因此，函数chooseBestSplit()只需完成两件事：用最佳方式切分数据集和生成相应的叶节点。 伪代码： 代码： 这样就可以构建一棵完整的回归树了。 4 模型树 下面将重用部分已有的树构建代码来创建一种新的树。该树仍然采用二元切分，但叶节点不再是简单的数值，取而代之的是一些线性模型。 用树来对数据建模，除了把叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数。 如上图所示，使用两条直线拟合的效果显然比一条直线的效果好，因此可以采用分段线性模型。 决策树相比于其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一颗大叔更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。 对于前面回归树的代码，需要修改两个地方。第一个是在叶节点生成线性模型而不是常数值。第二个是每次切分，误差的估计函数，这里稍加变化。对于给定数据集，应该先用线性的模型来对它进行拟合，然后计算真实的目标值与模型预测值间的差值。最后将这些差值的平方求和就得到了所需的误差。 模型树的叶节点生成函数 3 树剪枝一棵树如果节点过多，表明该模型可能对数据进行了“过拟合”，通过降低决策树的复杂度来避免过拟合的过程称为剪枝（pruning）。在前面chooseBestSplit()中的提前终止条件，实际上是在进行一种所谓的预剪枝(prepruning)操作。另一种形式的剪枝需要使用测试集和训练集，交叉验证来发现过拟合，称作后剪枝(postpruning)。 预剪枝树构建算法其实对输入的参数tolS和tolN非常敏感，如果使用其他值将不太容易达到这么好的效果。然后，通过不断修改停止条件来得到合理结果并不是很好的办法。事实上，我们尝尝甚至不确定到底需要寻找什么样的结果。这正是机器学习所关注的内容，计算机应该可以给出总体的概貌。 还有一种称为后剪枝的方法，可以利用测试集来对树进行剪枝。由于不需要用户指定参数，后剪枝是一种更理想化的剪枝方法。 后剪枝使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大，足够复杂，便于剪枝。接下来从上而下找到叶子节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并。 伪代码： 基于已有的树切分测试数据 如果存在任意子集是一棵树，则在该子集上递归剪枝过程 左右子集都剪枝完了之后，判断一下左右两边是否都已经是叶子结点了（可能原来就是，或者剪枝完了塌陷成叶子结点） 计算合并或者不合并的误差 如果合并会降低误差的话，就将叶节点合并* 5 示例树回归与标准回归的比较预测代码： 比较： 对于以上数据构建三个模型，回归树、模型树以及简单线性回归，并比较相关性系数。 可以看出该方法在R2值上面的表现不如两种树回归方法。所以，树回归在预测复杂数据时会比简单的线性模型更有效。 6 总结 Cart可以通过特征的选择迭代建立一颗分类树，使得每次的分类平面能最好的将剩余数据分为两类 $gini=1-\Sigma(pi^2)$，表示每个类别出现的概率和与1的差值分类问题：argmax（Gini-GiniLeft-GiniRight）回归问题：argmax(Var-VarLeft-VarRight)查找最佳特征f已经最佳属性阈值th，小于th的在左边，大于th的在右边子树分类回归树(Classification And Regression Tree)是一个决策二叉树，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：分类树：基尼指数最小化回归树：平方误差最小化 分类树：最终Cart选择GiniGain最小的特征作为划分特征 回归树： 以方差作为混乱程度，计算原始数据集D的方差Var(D)，更具某个特定特征以及特征值，将数据划分为左右两部分，样本在该特征上取值大于特征值的放到左边D1数据集，否则放到右边D2数据集，然后计算Var(D1)和Var(D2)。找出是的Var(D)-Var(D1)-Var(D2)最大的特征以及特征值。 关于剪枝：用独立的验证数据集对训练集生长的树进行剪枝（事后剪枝）。 能够处理大量特征的分类，并且还不用做特征选择 在训练完成之后能给出哪些feature的比较重要 训练速度很快 容易并行 实现相对简单 7 使用情况可以将连续的特征离散化ID3算法：处理离散值的量C45算法：处理连续值的量Cart算法：离散和连续 两者都合适？ 8 停止条件 直到每个叶子节点都只有一种类型的记录时停止，（这种方式很容易过拟合） 另一种是当叶子节点的记录树小于一定的阈值或者节点的信息增益小于一定的阈值时停止 9 决策树的分类与回归分类树 输出叶子节点中所属类别最多的那一类回归树 输出叶子节点中各个样本值的平均值模型树 输出每个叶子节点的线性回归模型 10 理想的决策树叶子节点数尽量少/深度尽量小(避免过拟合) 11 解决决策树的过拟合 剪枝 前置剪枝：在分裂节点的时候设计比较苛刻的条件，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果） （交叉验证）后置剪枝：在树建立完之后，用单个节点代替子树，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程） 随机森林 12 优缺点优点：计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征。缺点：单颗决策树分类能力弱，并且对连续值变量难以处理；容易过拟合。（后续出现了随机森林，减小了过拟合现象）。 三 无监督学习利用K-均值聚类算法对未标注数据分组使用Apriori算法进行关联分析使用FP-growth算法来高效发现频繁项集 参考机器学习实战第12章 优点：一般比Apriori要快缺点：实现比较困难，在某些数据集上性能会下降适用数据类型：标称类数据 EM最大期望算法EM用于隐含变量的概率模型的极大似然估计，它一般分为两步：第一步求期望(E),第二步求极大(M)，如果概率模型的变量都是观测变量，那么给定数据之后就可以直接使用极大似然法或者贝叶斯估计模型参数。但是当模型含有隐含变量的时候就不能简单的用这些方法来估计，EM就是一种含有隐含变量的概率模型参数的极大似然估计法。应用到的地方：混合高斯模型、混合朴素贝叶斯模型、因子分析模型 PageRank四 其他工具利用PCA来简化数据矩阵分解矩阵分解的概念：原始数据集$X\in R^{mxn}$可以分解成两个小的矩阵$U\in R^{mxk}$和$V\in ^{kxn}$使得$UV=X$那么我们可以找到一个矩阵$\Sigma\in R^{kxk}$,使得$U\Sigma\Sigma^{-1}V=X$，即将V矩阵的每一列都单位化，此时$U\Sigma V=X$然后，我们取$UV=\hat{X}\approx X$，就是X的近似了。并且它与原始数据X的距离$\left| X-UV \right|^2_F $最小化。 矩阵分解的作用： 数据恢复：矩阵是稀疏的、低秩的，因此不同维度数据之间是有关联的。 数据去噪：分解的另一个作用是可以只抽取其中信息量最多的特征，因为剩余特征往往是无用的噪声。 数据降维：矩阵分解之后得到更小的矩阵，可以在保留原有信息的同时，降低数据的存储空间。 发现数据内部隐含结构：通过矩阵分解之后，数据可以到达新的特征空间，在新的空间中往往具有更显著、高区分度的特征。 利用SVD简化数据 [ ] ToDo LDA [ ] ToDo 大数据和MapReduce五 编程语言和数学Python入门高等数学1 连续、可导与可微的关系偏导数连续是函数可微的充分条件，函数可微是函数可导和函数连续的充分条件，函数可导和函数连续无必然联系。 参考：函数可导与可微的直观联系 线性代数概率论复习1 高斯分布多维高斯概率密度函数： 参考：机器学习-周志华-附录 机器学习算法分类图]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[如何从零构建一个卷积神经网络分类器 - TensorFlow学习整理]]></title>
      <url>%2F2017%2F02%2F21%2F%E5%A6%82%E4%BD%95%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E7%B1%BB%E5%99%A8-TensorFlow%E5%AD%A6%E4%B9%A0%E6%95%B4%E7%90%86%2F</url>
      <content type="text"><![CDATA[&emsp; 深度学习 参考Deep Learning Tutorial TensorFlow TensorFlow是一个基于数据流图的深度学习框架 图中的节点表示数学运算，而图边表示在它们之间传递的张量（多维数组） TensorFlow的优点是架构非常灵活，允许你通过单一的API将计算部署到PC机，服务器或移动设备的一个或多个CPU或GPU 目前的版本是1.0，基于8GPU对Inception V3实现了7.3倍加速，以及基于64GPU对分布式Inception V3训练实现58倍加速。主要引入了以下新的特点： 提供Java和Go的实验API 引入了高级API模块：tf.contrib.learn，还包含一个全新的tf.keras模块，能够与高级神经网络库Keras完全兼容 发布了面向CPU和GPU的TensorFlow图形的特定领域编译器XLA的实验版本 生成TensorFlow Debugger(tfdbg)，一个用于调试实时TensorFlow程序的命令行界面和API 接下来我将结合如何从零开始构建一个卷积神经网络图像分类器来介绍并总结TensorFlow的使用经验 图像数据的创建 读取图片并保存为tfrecords文件 .tfrecords文件为TensorFlow特有的序列化，保存数据的文件。便于后续的大数据的读取。 训练时数据的读取 模型的定义 到上一步为止，我们已经可以获得用于训练的一个batch的examples和labels，其本质为numpy数组。如果你直接有numpy数组数据，也可以直接用于训练或预测。Tensorflow运行需要两步：1. 定义图的结构 2. 将操作放到一个会话中(Session)中运行。因此，在TensorFlow可以运行之前，我们必须先定义Graph，它是整个模型的结构。 TensorFlow模型主要有三个部分，和机器学习算法的三个主要部分对应。 Inference: 定义神经网络的输入，输出，隐含层单元以及权重，对于一个batch的输入，可以通过前馈计算，输出一个batch的logits(可以理解为每一类的输出大小)。 Loss: 通过logits和labels计算出当前输出结果的预测值与实际值之间差距的大小。通常使用的是softmax_cross_entropy Training: 通常使用的AdamOptimizer（自适应的随机梯度下降方法）。涉及到权重衰减，自适应学习率和动量等，使得模型不断趋向于最优解。不会限于局部最优且不会过冲。 Inference中用到的主要函数卷积层 tf.get_variable # 定义卷积核与偏置的初始权值 tf.add_to_collection # 添加变量到字典中 tf.nn.conv2d # 卷积 tf.nn.bias_add # 偏置 池化层 tf.nn.max_pool 全连接层 tf.get_variable # 定义全连接和偏置的初始权值 tf.reshape # 可能需要将最后池化层结果拉长成一个向量 tf.nn.relu_layer # 添加ReLU非线性模块 Dropout层 tf.nn.dropout # 在最后全连接层后加入dropout 输出层 tf.get_variable # 定义全连接和偏置的初识权值 tf.add(tf.matmul()) # 最后全连接方式输出logits 注意当模型复杂化的时候可以使用tf.variable_scope和tf.name_scope函数来共享参数。其中tf.variable_scope主要影响变量的命名，但默认会调用tf.name_scope，而tf.name_scope只会影响ops的命名。 Loss(logits, labels)中用到的主要函数: tf.reshape # 将labels reshape为[batch_size, 1]大小 tf.concat # 与同样大小的tf.range(0, batch_size)连接 tf.sparse_to_dense # 稀疏矩阵转为稠密矩阵 tf.nn.softmax_cross_entropy_with_logits # 计算交叉熵 tf.reduce_mean # 计算平均值 Training(loss, global_step)中用到的主要函数 tf.train.exponential_decay # 权值衰减设置函数 optimizer=tf.train.AdamOptimizer # 自适应梯度优化器 train_op=optimizer.minimize(loss, global_step=global_step) Evaluation(logits, labels) 预测评估中用到的主要函数 tf.nn.in_top_k # 计算每个实例的labels是否在logits预测的前k个类别中 tf.reduce_sum # 计算预测正确的数量 模型运行TensorFlow的几个重要元素变量和操作 Variable Constant Placeholder 其他各种OPs Session 相关函数 tf.global_variables_initializer() # 用于初始化所有变量 input_pipeline() # 从tfrecords读取数据 placeholder_inputs() # 定义输入 inference() loss() training() evaluation() coord = tf.train.Coordinator() # 用于驱动tfrecords数据的读取 threads = tf.train.start_queue_runners(sess=sess, coord=coord) Session.run() Tensorboard Tensorboard用于训练过程中相关信息的记录，便于可视化的分析和监视训练过程 相关函数 tf.summary.scalar # 定义各种标量信息，例如当前step的Accuracy, Loss等 tf.summary.image # 保存当前batch的图片 tf.summary.merge_all() # 将所有summary汇总为一个操作 summary_writer = tf.summary.FileWriter(log_dir,graph_def=sess.graph_def) # 定义一个写summary的句柄 summary_str = sess.run([summary_op], feed_dict=summary_feed) # 生成summary字符串 summary_writer.add_summary(summary_str[0], step) # 将summary字符串用句柄写入文件 模型保存与加载保存模型 saver = tf.train.Saver(max_to_keep=30) # 定义一个模型保存器 checkpoint_path = os.path.join(checkpoint_dir, &#39;model.ckpt&#39;) # 设置保存的位置 saver.save(sess, checkpoint_path, global_step=step) # 保存模型（需要Session, 位置以及当前训练的步数） 加载模型 ckpt = tf.train.get_checkpoint_state(checkpoint_dir=checkpoint_dir) # （通过路径找到保存的模型） saver.restore(sess, ckpt.model_checkpoint_path) # 加载模型]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python3.5 Anaconda3 Caffe深度学习框架搭建]]></title>
      <url>%2F2017%2F01%2F12%2FPython3-5-Anaconda3-Caffe%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%90%AD%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[&emsp; 前沿 &emsp;&emsp;本文主要介绍在Anaconda3和Python3.5的环境下搭建Caffe深度学习开发环境，主要创建了Caffe的Python接口。&emsp;&emsp;强调Python3.5和Anaconda3是因为，Caffe还没有全面支持Python3，并且默认不使用Anaconda。&emsp;&emsp;Python3.5是目前Python3的最新版本，而且安装过程相对比较复杂。我在安装的过程中前后折腾了很久，但同时也从中学到了许多从源码编译安装程序的知识。本文主要参考了以下博客，在此表示感谢（下文博客1即代表引用了下列的第1篇博客）： 在 python3.5 下使用 Caffe Using Caffe with Python3.5 Setting up a Deep Learning Machine from Scratch (Software) Ubuntu16.04+matlab2014a+anaconda2+OpenCV3.1+caffe安装 Install Caffe With Anaconda 准备环境ubuntu14.04系统可以输入cat /etc/issue查看ubuntu的版本 显卡驱动与CUDA安装主要参考博客1上的步骤，Nvidia显卡驱动安装可以参考Nvidia Drivers安装步骤，CUDA安装可以参考CUDA安装步骤，cuDNN安装可以参考cuDNN安装步骤 BLAS安装与配置BLAS（基础线性代数集合）是一个应用程序接口的标准。Caffe官网上推荐了三种实现：ATLAS, MKL, or OpenBLAS。其中atlas可以直接通过命令行安装（本文采用的就是这个），如果要使用intel的MKL库，可以参考博客2的中BLAS安装与配置步骤。 安装相关依赖包Caffe的编译依赖于很多C和C++的动态链接库，因此需要先用apt-get工具安装这些动态链接库。参考博客4主要步骤如下： 1sudo apt-get update&#10;&#10;sudo apt-get upgrade&#10;&#10;sudo apt-get install -y build-essential cmake git pkg-config&#10;&#10;sudo apt-get install -y libprotobuf-dev libleveldb-dev libsnappy-dev protobuf-compiler&#10;&#10;sudo apt-get install -y libatlas-base-dev &#10;&#10;sudo apt-get install -y --no-install-recommends libboost-all-dev&#10;&#10;sudo apt-get install -y libgflags-dev libgoogle-glog-dev liblmdb-dev 根据博客1这里有两个包的版本要注意：protobuf版本要3.0或以上版本libboost版本要1.55或以上版本下面介绍这两个包的具体配置 protobuf安装博客博客1提到使用apt-get安装的是2.0版本，不可以。因此需要到protobuf的release页面下载两个安装包： protobuf-cpp-3.0.0-beta-2.zip 或以上版本； protobuf-python-3.0.0-beta-2.zip 或以上版本。注意cpp和python的版本应该保持一致。 这里由于我在实际安装Caffe环境的时候发现protoc已经安装了，应该是之前用apt-get的时候安装的版本是可以的，并且使用protoc --version发现版本为3.0.0:所以应该是cpp这个包已经安装好了，我就只按照[^1]的步骤到release页面下载并安装了对于的3.0.0版本的python包，安装步骤如下： 1&#35299;&#21387;&#23433;&#35013;&#21253;&#65292;&#36827;&#20837;&#35299;&#21387;&#30340;&#30446;&#24405;&#10;$ cd python&#10;$ python setup.py build&#10;$ python setup.py test&#10;$ python setup.py install 这样protobuf python runtime就编译和安装好了。注意protobuf python runtime是作为pip的包安装的。但是你可以从conda里面看到他： 1$ conda list | grep protobuf 我这里貌似安装了两个版本的，不过有3.0.0版本的就可以了。 libboost安装按照博客1的解释，libboost安装完之后会产生两个版本的libboost_python: libboost_python-py33.so.XXX libboost_python-py34.so.XXX 而这里必须选择py34的动态链接库，否则在实际运行Caffe的时候可能会在得不到任何错误提示的情况下python kernel直接崩溃。 具体安装步骤可以参考博客1的步骤，这里有一个关键步骤必须要执行： 1sudo ln -s /usr/lib/x86_64-linux-gnu/libboost_python-py34.so.1.55.0 /usr/local/lib/libboost_python3.so 就是在/usr/local/lib目录下建立一个libboost_python3.so的软链接，而且到这里，需要配置一下.bashrc或者.zshrc的环境变量： 这里主要是在LD_LIBRARY_PATH环境变量中添加了Anaconda，Caffe的链接库路径以及/usr/local/lib目录，这样在编译的时候才能找到比如上面libboost_python3.so这样的动态链接库文件。 关于LD_LIBRARY_PATH环境变量以及ld.so.conf文件和ldconfig命令的使用，可以参考Linux 共享库 LD_LIBRARY_PATH 与ld.so.conf的使用ldconfig Anaconda3以及Caffe的编译和安装Anaconda3安装Anaconda3的安装比较简单，在Anaconda官网下载对于的Linux安装包（.sh文件）即可，安装命令为： 1bash Anaconda3-4.0.0-Linux-x86_64.sh 安装完Anaconda3之后可以参考博客4的建议，安装一下OpenCV包： 1conda install -c menpo opencv3 Caffe的编译和安装到这里就可以正式进行Caffe的编译和安装了，首先下载并解压，这一步大家都会，到github官网下载即可，解压后进入Caffe目录。 这里的主要步骤是Makefile.config文件的配置,首先运行cp Makefile.config.example Makefile.config创建一个Makefile.config，然后对其中内容进行更改，运行vim Makefile.config打开文件，进行如下修改： 其中有几个地方需要注意： USE_CUDNN:=1 # 取消注释后需要保证，在ld.so.conf文件中（记住ldconfig）或LD_LIBRARY_PATH环境变量中能找到CUDNN的动态链接库 OPENCV_VERSION:=3 # 如果安装了OpenCV3可以启用这一项 BLAS:=atlas # 如果使用别的MKL或者BLAS需要在下面几行配置目录 ANACONDA_HOME和PYTHON_INCLUDE # 按照我上面的修改即可 PYTHON_LIBRARIES # 同意确保boost_python3这个动态链接库在ld.so.conf文件中（记住ldconfig）或LD_LIBRARY_PATH中能找到。可以用locate boost_python命令看看动态链接库文件藏在哪里，然后把相应的目录添加到LD_LIBRARY_PATH环境变量中。 PYTHON_LIB # 需要启用 WITH_PYTHON_LAYER # 如果要使用Caffe的Python接口就需要启用 以上都配置完了之后，就可以编译Caffe以及Caffe的Python接口啦，参考[^2]可以使用多线程提高编译的速度： 1make all -j $(($(nproc) + 1))&#10;make test -j $(($(nproc) + 1))&#10;make runtest -j $(($(nproc) + 1))&#10;make pycaffe -j $(($(nproc) + 1)) 如果一切都顺利的话，到这里Caffe以及Caffe的Python接口已经编译完成并可以使用了。最后把Caffe的Python库的路径添加到PYTHONPATH环境变量中，这样在python或者ipython程序中才能import进来： PS：修改.bashrc或.zshrc之后记得source ~/.bashrc或者source ~/.zshrc一下才生效哦。 运行打开python或ipython已经可以正常使用caffe了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux常用命令整理]]></title>
      <url>%2F2016%2F12%2F28%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86%2F</url>
      <content type="text"><![CDATA[&emsp; 磁盘信息查看磁盘空间命令格式：df -h 以磁盘分区为单位查看文件系统 查看进程查看占用特定端口的进程 命令格式：lsof -i:6006 其中6006为想要查看的端口 根据进程号KILL特定进程 命令格式：kill -KILL 738 其中738为想要kill的进程的进程号]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[经典面试题] 最长回文子串]]></title>
      <url>%2F2016%2F12%2F21%2F%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E9%A2%98-%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%2F</url>
      <content type="text"><![CDATA[本文主要参考微信公众号(待字闺中)推文”【经典面试题】最长回文” 整理而成，在此特别感谢。 题目LeetCode 5. Longest Palindromic Substring Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example: Input: “babad” Output: “bab” Note: “aba” is also a valid answer.Example: Input: “cbbd” Output: “bb” &emsp; 解题思路1. 暴力解法在外层使用2重循环找出所有子串，内层循环判断当前子串是否为回文子串。 时间复杂度为$O(n^3)$，空间复杂度为$O(1)$ 2. 动态规划方法开辟一个二维数组p[i][j]用来表示str[i…j]是否为回文子串，p[i][j]的状态转移方程为： i==j时，p[i][j]=true i+1==j时，p[i][j]=(str[i]==str[j]) 其他情况,p[i][j]=(str[i]==str[j] &amp;&amp; p[i+1]==p[j-1]) 时间复杂度为$O(n^2)$，空间复杂度为$O(n^2)$, 时间上比暴力法有较大优化。 3. 确定中心后像两边扩展可以先确定一个字符为中心，然后向两边扩展，需要注意的是要同时考虑奇数和偶数的情况。 时间复杂度为$O(n^2)$，空间复杂度为$O(1)$ 4. 后缀数组可以利用后缀数组，将最长回文子串问题转换为求后缀数组中最长公共前缀的问题。具体做法是将字符串反转，拼接到原字符之后，中间注意用特殊字符隔开。然后求新字符串的后缀数组的任意两个字符串的最大公共前缀。 例如原字符串s0=”abcbd”，那转换后编程s1=”abcbd#dbcba”，则后缀数组为：“a”“ba”“cba”“bcba”“dbcba”“#dbcba”“d#dbcba”“bd#dbcba”“cbd#dbcba”“bcbd#dbcba”“abcbd#dbcba”可以看出，字符串”bcba”和“bcbd#dbcba”的最大公共前缀=3，长度最长。 具体实现算法好坏可能导致复杂度差异很大。 5. Manacher算法Manacher算法是一种接近线性时间的算法，在上面方法3中需要考虑回文长度为奇数和偶数的情况，这里引入一个技巧，通过在字符之间插入特殊字符（例如#字符）可以统一处理。例如字符串“abba”，插入#字符后转换为“#a#b#b#a#”，然后创建一个数组p，计算以当前第i个字符为中心的回文串的半径。可以发现任意一个回文串的长度都是奇数，而且该回文串包含的原字符串中的字符个数=p[i]-1，且这些原字符一定构成回文串。 具体的算法详解可以参考微信公众号(待字闺中)推文”【经典面试题】最长回文” 和hdu3068之manacher算法+详解这两篇博客内容。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[LeetCode] Template]]></title>
      <url>%2F2016%2F12%2F17%2FLeetCode-Template%2F</url>
      <content type="text"><![CDATA[&emsp; title: ‘[LeetCode] 174. Dungeon Game’date: 2016-09-11 18:00:53categories: LeetCode tags: [LeetCode, Dynamic Programming, String, Stack]题目描述:174. Dungeon GameThe demons had captured the princess (P) and imprisoned her in the bottom-right corner of a dungeon. The dungeon consists of M x N rooms laid out in a 2D grid. Our valiant knight (K) was initially positioned in the top-left room and must fight his way through the dungeon to rescue the princess. The knight has an initial health point represented by a positive integer. If at any point his health point drops to 0 or below, he dies immediately. Some of the rooms are guarded by demons, so the knight loses health (negative integers) upon entering these rooms; other rooms are either empty (0’s) or contain magic orbs that increase the knight’s health (positive integers). In order to reach the princess as quickly as possible, the knight decides to move only rightward or downward in each step. Write a function to determine the knight’s minimum initial health so that he is able to rescue the princess. For example, given the dungeon below, the initial health of the knight must be at least 7 if he follows the optimal path RIGHT-&gt; RIGHT -&gt; DOWN -&gt; DOWN. Notes:The knight’s health has no upper bound.Any room can contain threats or power-ups, even the first room the knight enters and the bottom-right room where the princess is imprisoned. 题目大意：骑士需要从左上角开始进入房间，每次向右或向下移动一步进入下一个房间，直到到达右下角的房间救出公主。每个房间可能由精灵把守，会造成骑士失血，用负数表示；可能为空，用0表示；也可能有魔法球，可以给骑士补血。从左上角第一个房间开始，直到进入右下角关押公主的房间，血量都不能&lt;=0。 解题思路：这是一道二维的动态规划问题，用DP[i][j]表示状态，表示接下来骑士进入(i,j)房间后能救出公主最少需要具备多少血。状态转移方程为: $ dp[i][j] = max(min(dp[i+1][j], dp[i][j+1])-dungeon[i][j], 1) $ (1) 选择向右或者向下中代价最小的方向，然后根据当前要进入的房间(i,j)相应调整需要的血量。 若$dungeon[i][j] &lt;= 0$,则$dp[i][j] = min(dp[i+1][j], dp[i][j+1])-dungeon[i][j]$ (2) 若$dungeon[i][j] &gt; 0$,则$dp[i][j] = max(min(dp[i+1][j], dp[i][j+1])-dungeon[i][j], 1) $ (3) (2)、（3）式可以统一为（1）式子。 C++代码：1class Solution &#123;&#10;public:&#10; int calculateMinimumHP(vector&#60;vector&#60;int&#62;&#62;&#38; dungeon) &#123;&#10; int rows = dungeon.size();&#10; int cols = dungeon[0].size();&#10; vector&#60;vector&#60;int&#62;&#62; dp(rows, vector&#60;int&#62;(cols, 0));&#10; dp[rows-1][cols-1] = max(1-dungeon[rows-1][cols-1], 1);&#10; &#10; for (int i=rows-2; i&#62;=0; i--) &#123;&#10; dp[i][cols-1] = max(dp[i+1][cols-1]-dungeon[i][cols-1], 1);&#10; &#125;&#10; for (int j=cols-2; j&#62;=0; j--) &#123;&#10; dp[rows-1][j] = max(dp[rows-1][j+1]-dungeon[rows-1][j], 1);&#10; &#125;&#10; for (int i=rows-2; i&#62;=0; i--) &#123;&#10; for (int j=cols-2; j&#62;=0; j--) &#123;&#10; dp[i][j] = max(min(dp[i+1][j], dp[i][j+1])-dungeon[i][j], 1);&#10; &#125;&#10; &#125;&#10; return dp[0][0];&#10; &#125;&#10;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[用Anaconda安装配置Jupyter Notebook和TensorFlow开发环境]]></title>
      <url>%2F2016%2F09%2F15%2F%E7%94%A8Anaconda%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEJupyter-Notebook%E5%92%8CTensorFlow%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
      <content type="text"><![CDATA[&emsp; Anaconda安装在Anaconda官网下载并按照指令安装。 建议安装Python3版本，我在ubuntu14上测试发现安装Python2版本一直有IPython console can’t locate “backports.shutil_get_terminal_size” and won’t load的错误，按照stackoverflow上面的方法尝试也无法解决。之后看到有人评论说Python2目前已经过时了，于是重新安装了Python3版本的Anaconda后解决。 TensorFlow安装包管理在Anaconda中可以使用conda或者pip进行包管理，可以查看conda常用命令和pip常用命令。conda可以看做是pip和virtualenv的集成，三者的常见命令对比可以查看conda vs. pip vs. virtualenv)。 安装包命令网速慢解决方法使用pip安装离线下载的whl格式安装包&emsp;&emsp;可以从PyPI Python安装包中心搜索并下载相应的按照包之后在本地离线安装。&emsp;&emsp;例如，在下载名为kivy.deps.gstreamer的包kivy.deps.gstreamer-0.1.9-cp27-cp27m-win_amd64.whl，则安装命令为： 1pip install kivy.deps.gstreamer-0.1.9-cp27-cp27m-win_amd64.whl 使用shadowsocks)+privoxy+proxychains-ng搭建局域网翻墙代理 &emsp;&emsp;这适用于服务器不适合直接搭建shadowsocks翻墙的情况，如果可以直接用shadowsocks翻墙，自然不需要这样做了。&emsp;&emsp;我的情况是自己的Mac笔记本可以使用shadowsocks[以下简称SS]或者鱼摆摆进行翻墙，而ubuntu服务器无法翻墙。因此，我在Mac上用SS或者鱼摆摆翻墙，然后用privoxy与之相连，在本地建立一个局域网http翻墙代理，最后在ubuntu服务器上使用proxychains-ng代理到Mac上实现翻墙来安装包。 具体的配置可以参看下面两篇博客： &emsp;&emsp;实际上我在配置的过程中，用privoxy代理到鱼摆摆的本地http代理端口可以正常使用，但是代理到socks5端口就不行，具体原因不太清楚。 privoxy配置 proxycahins-ng配置 安装TensorFlow&emsp;&emsp;直接参考TensorFlow官网的安装教程即可， 注意显卡驱动，Cuda Toolkit以及cuDNN需要正确的配置。具体可以参考官网或者github上的深度学习框架搭建博客，其中包含了各种深度学习框架的搭建方法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[LeetCode] 32. Longest Valid Parentheses]]></title>
      <url>%2F2016%2F09%2F12%2FLeetCode-32-Longest-Valid-Parentheses%2F</url>
      <content type="text"><![CDATA[&emsp; 题目描述:32. Longest Valid Parentheses Given a string containing just the characters ‘(‘ and ‘)’, find the length of the longest valid (well-formed) parentheses substring. For “(()”, the longest valid parentheses substring is “()”, which has length = 2. Another example is “)()())”, where the longest valid parentheses substring is “()()”, which has length = 4. Subscribe to see which companies asked this question 题目大意：给定一个仅包含’(‘和’)’的字符串，找出最长有效括号匹配子串的长度。 对于”(()”, 最长的有效括号匹配子串为”()”,长度为2。 另一个例子是”)()())”,最长的有效括号匹配子串为”()()”, 长度为4。 解题思路：解法1 栈（Stack）引用克林32. Longest Valid Parentheses的思路。括号匹配经常会用到栈，当碰到做括号’(‘时，将字符压入栈中，当碰到’)’字符时查看栈顶元素，若栈顶元素为’(‘，则将栈顶元素弹出，否则栈为空或者栈顶元素为’)’，匹配失败，将其压入栈中。因此，最终遍历完整个字符串后栈中剩下的元素就是无法匹配的字符，而它们间隔开的就是所有括号有效匹配的字串。因此，在入栈的时候需要加入当前字符的索引，最后一次出栈的时候可以计算中间的间隔字符数量，最大的即是结果。 C++代码：1struct PT &#123;&#10; char ch;&#10; int pos;&#10; PT(char ch_, int pos_):ch(ch_),pos(pos_)&#123;&#125;&#10;&#125;;&#10;&#10;class Solution &#123;&#10;public:&#10; int longestValidParentheses(string s) &#123;&#10; if (s.empty()) return 0;&#10; stack&#60;PT&#62; sk;&#10; for (int i = 0; i &#60; s.length(); i++) &#123;&#10; int character = s[i];&#10; if (character == &#39;(&#39;) &#123;&#10; sk.push(PT(&#39;(&#39;, i));&#10; &#125; else &#123;&#10; if (!sk.empty() &#38;&#38; sk.top().ch==&#39;(&#39;) &#123;&#10; sk.pop();&#10; &#125; else &#123;&#10; sk.push(PT(&#39;)&#39;,i));&#10; &#125;&#10; &#125;&#10; &#125;&#10; int ans = 0;&#10; int pre_pos = s.length();&#10; while (!sk.empty()) &#123;&#10; int now_pos = sk.top().pos;&#10; sk.pop();&#10; ans = max(ans, pre_pos-now_pos - 1);&#10; pre_pos = now_pos;&#10; &#125;&#10; ans = max(ans, pre_pos);&#10; return ans;&#10; &#125;&#10;&#125;; 解法2 动态规划(Dynamic Programming)时间复杂度O(N),空间复杂度O(N)参考https://segmentfault.com/a/1190000003481194的思路还可以用动态规划的方法，状态dp[i]表示从下标i开始直到字符串结尾最长括号对长度，s[i]是字符串下标为i的括号。如果s[i]是左括号，而且i+d[i+1]+1是右括号的话，那么d[i] == d[i+1]+1，如果不是则为0。如果s[i]是右括号，因为没有右括号开头的括号对，因此d[i]=0。此外还需要加上dp[i+dp[i+1]+1+1)。 C++代码1public class Solution &#123;&#10; public int longestValidParentheses(String s) &#123;&#10; int[] dp = new int[s.length()];&#10; int maxLen = 0;&#10; for(int i = s.length()-2; i &#62;=0; i--)&#123;&#10; if(s.charAt(i)==&#39;(&#39;)&#123;&#10; int end = i + dp[i+1] + 1;&#10; if(end &#60; s.length() &#38;&#38; s.charAt(end)==&#39;)&#39;)&#123;&#10; dp[i] = dp[i+1] + 2;&#10; if(end + 1 &#60; s.length())&#123;&#10; dp[i] += dp[end + 1];&#10; &#125;&#10; &#125;&#10; &#125;&#10; maxLen = Math.max(maxLen, dp[i]);&#10; &#125;&#10; return maxLen;&#10; &#125;&#10;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[LeetCode] 312. Burst Balloons]]></title>
      <url>%2F2016%2F09%2F11%2FLeetCode-312-Burst-Balloons%2F</url>
      <content type="text"><![CDATA[&emsp; 题目描述:312. Burst BalloonsGiven n balloons, indexed from 0 to n-1. Each balloon is painted with a number on it represented by array nums. You are asked to burst all the balloons. If the you burst balloon i you will get nums[left] nums[i] nums[right] coins. Here left and right are adjacent indices of i. After the burst, the left and right then becomes adjacent. Find the maximum coins you can collect by bursting the balloons wisely. Note:(1) You may imagine nums[-1] = nums[n] = 1. They are not real therefore you can not burst them.(2) 0 ≤ n ≤ 500, 0 ≤ nums[i] ≤ 100 Example:Given [3, 1, 5, 8] Return 167 nums = [3,1,5,8] –&gt; [3,5,8] –&gt; [3,8] –&gt; [8] –&gt; [] coins = 3 1 5 + 3 5 8 + 1 3 8 + 1 8 1 = 167 题目大意：给定n个气球，下标为0到n-1。每个气球上都标有一个数字，用数组nums表示。你被要求扎破所有气球。扎破第i个气球可以获得nums[left] nums[i] nums[right]枚硬币。这里left和right是与i相邻的下标。扎破气球以后，left和right就变成相邻的了。 寻找最优策略下可以获得的硬币数。 解题思路： 动态规划（Dynamic Programming）时间复杂度O(N^3)题目需要采用逆向思维，整个题目可以理解为将一个区间的的气球依次炸掉获得硬币，可以控制最后炸掉哪个气球。状态dp[l][r]表示需要爆炸的区间，以最后一个爆破的气球m为界限，可以把问题转换为左右两个区域的子问题。状态转移方程：dp[l][r]表示扎破（l,r)范围内所有气球获得的最大硬币数。 l与r的跨度k从2开始逐渐增大。 C++代码：java版代码引用书影博客[LeetCode]Burst Balloons 1class Solution &#123;&#10;public:&#10; int maxCoins(vector&#60;int&#62;&#38; nums) &#123;&#10; int sz = nums.size();&#10; vector&#60;vector&#60;int&#62;&#62; cnt(sz, vector&#60;int&#62;(sz, 0));&#10; return maxC(nums, 0, nums.size() - 1, 1, 1, cnt);&#10; &#125;&#10; int maxC(vector&#60;int&#62;&#38; nums, int from, int to, int left, int right, vector&#60;vector&#60;int&#62;&#62;&#38; cnt) &#123;&#10; if (from &#62; to) return 0;&#10; if (cnt[from][to]) return cnt[from][to];&#10; if (from == to &#38;&#38; from &#62;= 0 &#38;&#38; from &#60; nums.size()) &#123;&#10; cnt[from][to] = nums[from] * left * right;&#10; return nums[from] * left * right;&#10; &#125;&#10; &#10; int max_num = -1;&#10; for (int i = from; i &#60; nums.size() &#38;&#38; i &#60;= to; i++) &#123;&#10; max_num = max(max_num, maxC(nums, from, i-1, left, nums[i], cnt)&#10; + maxC(nums, i+1, to, nums[i], right, cnt) + nums[i] * left * right);&#10; &#125;&#10; cnt[from][to] = max_num;&#10; return max_num;&#10; &#125;&#10;&#125;;&#10;&#10;public class Solution &#123;&#10; public int maxCoins(int[] iNums) &#123;&#10; int[] nums = new int[iNums.length + 2];&#10; int n = 1;&#10; for (int x : iNums) if (x &#62; 0) nums[n++] = x;&#10; nums[0] = nums[n++] = 1;&#10;&#10; int[][] dp = new int[n][n];&#10; for (int k = 2; k &#60; n; ++k)&#10; for (int l = 0; l &#60; n - k; ++l) &#123;&#10; int r = l + k;&#10; for (int m = l + 1; m &#60; r; ++m)&#10; dp[l][r] = Math.max(dp[l][r], &#10; nums[l] * nums[m] * nums[r] + dp[l][m] + dp[m][r]);&#10; &#125;&#10; &#10; return dp[0][n - 1];&#10; &#125;&#10;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[LeetCode] 354. Russian Doll Envelopes]]></title>
      <url>%2F2016%2F09%2F11%2FLeetCode-354-Russian-Doll-Envelopes%2F</url>
      <content type="text"><![CDATA[&emsp; 题目描述:354. Russian Doll Envelopes You have a number of envelopes with widths and heights given as a pair of integers (w, h). One envelope can fit into another if and only if both the width and height of one envelope is greater than the width and height of the other envelope. What is the maximum number of envelopes can you Russian doll? (put one inside other) Example:Given envelopes = [[5,4],[6,4],[6,7],[2,3]], the maximum number of envelopes you can Russian doll is 3 ([2,3] =&gt; [5,4] =&gt; [6,7]). 题目大意：题意比较简单，给定一定量不同长宽的信封，小的信封可以放在大的信封内部，问最多能嵌套放几个。 解题思路：首先将信封(w,h)按照按照宽度排序，宽度小的在宽度大的后面，若相等，则长度小的在前，这样排在后面的信封不可能包含在前面的信封之中。 动态规划的思想也从中而来，状态DP[i]表示第i个信封所包含的信封的数量。从上面可知，若一个信封包含其他的信封(DP[i]&gt;1)则，它包含的一定是排在它之前的某个信封，即状态转移方程为 对每个信封i，初始化： $DP[i] = 1$ (1)对于i&gt;=2， C++代码：1struct cmp &#123;&#10; bool operator()(pair&#60;int,int&#62;&#38; a, pair&#60;int,int&#62;&#38; b) &#123;&#10; return b.first &#60; a.first || (b.first==a.first &#38;&#38; b.second &#60;a.second);&#10; &#125; &#10;&#125;;&#10;class Solution &#123;&#10;public:&#10; int maxEnvelopes(vector&#60;pair&#60;int, int&#62;&#62;&#38; envelopes) &#123;&#10; sort(envelopes.begin(), envelopes.end());&#10; int n = envelopes.size();&#10; if (n &#60;= 1) return n;&#10; &#10; int ans = INT_MIN;&#10; vector&#60;int&#62; dp(n, 1);&#10; vector&#60;int&#62; pre(n, -1);&#10; for (int i=1; i&#60;n; i++) &#123;&#10; for (int j=0; j&#60;i; j++) &#123;&#10; if (canContain(envelopes[i], envelopes[j]) &#38;&#38; dp[j]+1&#62;dp[i]) &#123;&#10; dp[i] = dp[j]+1;&#10; pre[i] = j;&#10; &#125;&#10; &#125;&#10; &#125;&#10; &#10; for (int i=0; i&#60;n; i++) &#123;&#10; ans = max(ans, dp[i]);&#10; &#125;&#10; return ans;&#10; &#125;&#10; &#10; bool canContain(pair&#60;int,int&#62;&#38; a, pair&#60;int,int&#62;&#38; b) &#123;&#10; return a.first &#62; b.first &#38;&#38; a.second &#62; b.second;&#10; &#125;&#10;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[LeetCode] 174. Dungeon Game]]></title>
      <url>%2F2016%2F09%2F11%2FLeetCode-174-Dungeon-Game%2F</url>
      <content type="text"><![CDATA[&emsp; 题目描述:174. Dungeon GameThe demons had captured the princess (P) and imprisoned her in the bottom-right corner of a dungeon. The dungeon consists of M x N rooms laid out in a 2D grid. Our valiant knight (K) was initially positioned in the top-left room and must fight his way through the dungeon to rescue the princess. The knight has an initial health point represented by a positive integer. If at any point his health point drops to 0 or below, he dies immediately. Some of the rooms are guarded by demons, so the knight loses health (negative integers) upon entering these rooms; other rooms are either empty (0’s) or contain magic orbs that increase the knight’s health (positive integers). In order to reach the princess as quickly as possible, the knight decides to move only rightward or downward in each step. Write a function to determine the knight’s minimum initial health so that he is able to rescue the princess. For example, given the dungeon below, the initial health of the knight must be at least 7 if he follows the optimal path RIGHT-&gt; RIGHT -&gt; DOWN -&gt; DOWN. Notes:The knight’s health has no upper bound.Any room can contain threats or power-ups, even the first room the knight enters and the bottom-right room where the princess is imprisoned. 题目大意：骑士需要从左上角开始进入房间，每次向右或向下移动一步进入下一个房间，直到到达右下角的房间救出公主。每个房间可能由精灵把守，会造成骑士失血，用负数表示；可能为空，用0表示；也可能有魔法球，可以给骑士补血。从左上角第一个房间开始，直到进入右下角关押公主的房间，血量都不能&lt;=0。 解题思路：这是一道二维的动态规划问题，用DP[i][j]表示状态，表示接下来骑士进入(i,j)房间后能救出公主最少需要具备多少血。状态转移方程为: $ dp[i][j] = max(min(dp[i+1][j], dp[i][j+1])-dungeon[i][j], 1) $ (1) 选择向右或者向下中代价最小的方向，然后根据当前要进入的房间(i,j)相应调整需要的血量。 若$dungeon[i][j] &lt;= 0$,则$dp[i][j] = min(dp[i+1][j], dp[i][j+1])-dungeon[i][j]$ (2) 若$dungeon[i][j] &gt; 0$,则$dp[i][j] = max(min(dp[i+1][j], dp[i][j+1])-dungeon[i][j], 1) $ (3) (2)、（3）式可以统一为（1）式子。 C++代码：1class Solution &#123;&#10;public:&#10; int calculateMinimumHP(vector&#60;vector&#60;int&#62;&#62;&#38; dungeon) &#123;&#10; int rows = dungeon.size();&#10; int cols = dungeon[0].size();&#10; vector&#60;vector&#60;int&#62;&#62; dp(rows, vector&#60;int&#62;(cols, 0));&#10; dp[rows-1][cols-1] = max(1-dungeon[rows-1][cols-1], 1);&#10; &#10; for (int i=rows-2; i&#62;=0; i--) &#123;&#10; dp[i][cols-1] = max(dp[i+1][cols-1]-dungeon[i][cols-1], 1);&#10; &#125;&#10; for (int j=cols-2; j&#62;=0; j--) &#123;&#10; dp[rows-1][j] = max(dp[rows-1][j+1]-dungeon[rows-1][j], 1);&#10; &#125;&#10; for (int i=rows-2; i&#62;=0; i--) &#123;&#10; for (int j=cols-2; j&#62;=0; j--) &#123;&#10; dp[i][j] = max(min(dp[i+1][j], dp[i][j+1])-dungeon[i][j], 1);&#10; &#125;&#10; &#125;&#10; return dp[0][0];&#10; &#125;&#10;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LeetCode 解题报告]]></title>
      <url>%2F2016%2F09%2F10%2FLeetCode-%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A%2F</url>
      <content type="text"><![CDATA[&emsp; # 解题报告 分类 难度 32 Longest Valid Parentheses Dynamic Programming Stack String Hard 174 Dungeon Game Dynamic Programming Hard 312 Burst Balloons Dynamic Programming Hard 354 Russian Doll Envelopes Dynamic Programming Hard 377 Combination Sum Dynamic Programming Medium]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[天使的模样]]></title>
      <url>%2F2016%2F09%2F10%2FPost-with-cover-image%2F</url>
      <content type="text"><![CDATA[&emsp; 女孩的名叫海莉·福特，今年九岁，来自华盛顿州布雷默顿，与她小小的年龄不相称的是，她正举着钉枪，怀着大大的心意，在给流浪者建造庇护所。海莉说：“我觉得每个人都应该有一个住的地方，还有人无家可归，我得给他们建造房子。”目前，在妈妈和祖父的帮助下，海莉在社区里给无家可归的人们建造第一座木房子，希望能靠自己的努力来改变流浪汉们的生活。 海莉为流浪汉们做公益已经有四年的时间了。那还是四年前，她她与妈妈米兰达逛完杂货铺出来，遇到了在当地丢失工作的流浪汉爱德华，他坐在路边，目光呆滞，已经很多天没吃饱饭了，饿得奄奄一息，五岁的海莉动了恻隐之心，跟妈妈商量着给爱德华买了个三明治。爱德华高兴得当场落泪，多年的流浪生涯让他看透了人情冷暖，世态的炎凉，而这个小女孩给了他温暖的一天。 在街上，海莉又遇到了另一个流浪汉比利·雷，一个退伍军人，在战争中失去了双腿，无家可归的他再次激起了海莉的爱心。帮完了一个又一个流浪汉后，母亲告诉她：“我们并不是多么富裕，没有能力帮助所有的人。”小女孩的眼角挂着泪水，倔强地看着母亲，她没有绝望，任性地说：“不！我要试试看！”买不起食物，她就自己去种。 就这样，这个善良的小女孩开始挖地，播种，除草，建栅栏，她要自己培育果蔬，为更多无家可归的人提供食物。没有种植经验，她就在晚上一个人抱着厚厚的书本学习，功夫不负有心人，她终于收获55磅的果蔬，悉数捐出，这可以满足流浪者一年的食量。如今，她的目标是收获250磅的食物，并通过自己动手，为流浪者建造庇护所，她拿起刀子，扛起电钻她要为流浪者建一个遮风避雨的小屋，带着口罩认真的粉刷墙壁。 为了资助海莉的项目，妈妈米兰达向一家非营利性组织申请了一笔补助，海莉收到了3000美元的赠款。当地的一家建材商店劳氏商店承诺，海莉来采购建屋的材料能最低打五折。 海莉的行动触动了一大批志愿者加入。一些爱心人士也纷纷伸出援助之手，给予海莉力所能及的帮助。一位女士，为那些女流浪者捐赠了女性用品，她将物品交给了海莉，并且告诉她“不够再来拿”。一家慈善机构知道海莉的故事后，给她捐了3000美元。她将这些钱用来购买一些生活用品，在一个圣诞节送给了流浪汉的孩子，“我看到了他们每个人都特别高兴，他们告诉我说我就是他们的圣诞老人。” 海莉的善举，成为了一股推动改善流浪汉生活的力量。在海莉的影响下，有人开始捐钱，有人开始捐种子，更多的人开始关注身边的流浪者，他们和这些无家可归的人聊天，尽可能的帮助他们。“不应该有人在街头流浪，流浪者们需要更多人的帮助！”海莉的故事正在改变世界流浪汉的现状。 尽自己最大的努力，怀揣一颗善良的心，去帮助那些需要帮助的人，这个世界就会因为你的小小善举而在慢慢发生改变。如果世界上真的有天使，那么天使应该就是这个女孩的模样。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[LeetCode] 377. Combination Sum]]></title>
      <url>%2F2016%2F09%2F10%2FLeetCode-377-Combination-Sum%2F</url>
      <content type="text"><![CDATA[&emsp; 题目描述:377. Combination Sum IVGiven an integer array with all positive numbers and no duplicates, find the number of possible combinations that add up to a positive integer target. Example: nums = [1, 2, 3]target = 4 The possible combination ways are:(1, 1, 1, 1)(1, 1, 2)(1, 2, 1)(1, 3)(2, 1, 1)(2, 2)(3, 1) Note that different sequences are counted as different combinations. Therefore the output is 7. Follow up:What if negative numbers are allowed in the given array?How does it change the problem?What limitation we need to add to the question to allow negative numbers? 题目大意：给定一个无重复的正整数数组，计算得到一个目标正整数的所有可能组合方式的个数。 测试用例见题目描述。 注意不同的序列顺序应当视为不同的组合。 进一步思考：给定一个无重复的正整数数组，计算得到一个目标正整数的所有可能组合方式的个数。 测试用例见题目描述。 注意不同的序列顺序应当视为不同的组合。 解题思路：动态规划（Dynamic Programming） 状态转移方程：dp[x + y] += dp[x] 其中dp[x]表示生成数字x的所有可能的组合方式的个数。 Python代码：1class Solution(object):&#10; def combinationSum4(self, nums, target):&#10; &#34;&#34;&#34;&#10; :type nums: List[int]&#10; :type target: int&#10; :rtype: int&#10; &#34;&#34;&#34;&#10; dp = [0] * (target + 1)&#10; dp[0] = 1&#10; for x in range(target + 1):&#10; for y in nums:&#10; if x + y &#60;= target:&#10; dp[x + y] += dp[x]&#10; return dp[target]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[图片博客模板]]></title>
      <url>%2F2016%2F06%2F09%2F%E5%9B%BE%E7%89%87%E5%8D%9A%E5%AE%A2%E6%A8%A1%E6%9D%BF%2F</url>
      <content type="text"><![CDATA[]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Jupyter Notebook 安装扩展插件]]></title>
      <url>%2F2016%2F05%2F29%2FJupyter-Notebook-%E5%AE%89%E8%A3%85%E6%89%A9%E5%B1%95%E6%8F%92%E4%BB%B6%2F</url>
      <content type="text"><![CDATA[&emsp; 更新pip 最简单的安装方法是使用pip，首先将pip更新到最新版1pip install --upgrade pip 安装Jupyter Notebook 扩展 安装方法可以参考GitHub Jupyter Notebook主页上的README.md文档，最简单的方法是pip install 1pip install https://github.com/ipython-contrib/IPython-notebook-extensions/archive/master.zip --user 注意事项 网络不好的时候可以选择离线安装 需要将anaconda目录添加读写权限]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[文字博客模板]]></title>
      <url>%2F2016%2F05%2F25%2F%E6%96%87%E5%AD%97%E5%8D%9A%E5%AE%A2%E6%A8%A1%E6%9D%BF%2F</url>
      <content type="text"><![CDATA[This is a test template]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F05%2F25%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
